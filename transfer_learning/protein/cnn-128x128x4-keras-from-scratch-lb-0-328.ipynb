{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "b3fd4b30584e8fdf544f23d1a56028fa5fa05d1f"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SEED = 777\n",
    "SHAPE = (192, 192, 4)\n",
    "DIR = '.'\n",
    "VAL_RATIO = 0.1 # 10 % as validation\n",
    "THRESHOLD = 0.05 # due to different cost of True Positive vs False Positive, this is the probability threshold to predict the class as 'yes'\n",
    "\n",
    "ia.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "fd1045b817e77ff540e789ea3b0eea22bcdb9a6f"
   },
   "outputs": [],
   "source": [
    "def getTrainDataset():\n",
    "    \n",
    "    path_to_train = DIR + '/train/'\n",
    "    data = pd.read_csv(DIR + '/train.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "        y = np.zeros(28)\n",
    "        for key in lbl:\n",
    "            y[int(key)] = 1\n",
    "        paths.append(os.path.join(path_to_train, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "\n",
    "def getTestDataset():\n",
    "    \n",
    "    path_to_test = DIR + '/test/'\n",
    "    data = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name in data['Id']:\n",
    "        y = np.ones(28)\n",
    "        paths.append(os.path.join(path_to_test, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# credits: https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py#L302\n",
    "# credits: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "class ProteinDataGenerator(keras.utils.Sequence):\n",
    "            \n",
    "    def __init__(self, paths, labels, batch_size, shape, shuffle = False, use_cache = False, augment = False):\n",
    "        self.paths, self.labels = paths, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shape = shape\n",
    "        self.shuffle = shuffle\n",
    "        self.use_cache = use_cache\n",
    "        self.augment = augment\n",
    "        if use_cache == True:\n",
    "            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], shape[2]), dtype=np.float16)\n",
    "            self.is_cached = np.zeros((paths.shape[0]))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        paths = self.paths[indexes]\n",
    "        X = np.zeros((paths.shape[0], self.shape[0], self.shape[1], self.shape[2]))\n",
    "        # Generate data\n",
    "        if self.use_cache == True:\n",
    "            X = self.cache[indexes]\n",
    "            for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n",
    "                image = self.__load_image(path)\n",
    "                self.is_cached[indexes[i]] = 1\n",
    "                self.cache[indexes[i]] = image\n",
    "                X[i] = image\n",
    "        else:\n",
    "            for i, path in enumerate(paths):\n",
    "                X[i] = self.__load_image(path)\n",
    "\n",
    "        y = self.labels[indexes]\n",
    "                \n",
    "        if self.augment == True:\n",
    "            seq = iaa.Sequential([\n",
    "                iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Strengthen or weaken the contrast in each image.\n",
    "                    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "                    # Add gaussian noise.\n",
    "                    # For 50% of all images, we sample the noise once per pixel.\n",
    "                    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "                    # channel. This can change the color (not only brightness) of the\n",
    "                    # pixels.\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-8, 8)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "            X = np.concatenate((X, seq.augment_images(X), seq.augment_images(X), seq.augment_images(X)), 0)\n",
    "            y = np.concatenate((y, y, y, y), 0)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.paths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "            \n",
    "    def __load_image(self, path):\n",
    "        R = Image.open(path + '_red.png')\n",
    "        G = Image.open(path + '_green.png')\n",
    "        B = Image.open(path + '_blue.png')\n",
    "        Y = Image.open(path + '_yellow.png')\n",
    "\n",
    "        im = np.stack((\n",
    "            np.array(R), \n",
    "            np.array(G), \n",
    "            np.array(B),\n",
    "            np.array(Y)), -1)\n",
    "        \n",
    "        im = cv2.resize(im, (SHAPE[0], SHAPE[1]))\n",
    "        im = np.divide(im, 255)\n",
    "        return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97a72f6757c7c2a0e6c8116eb604d185dc6c64ed"
   },
   "source": [
    "# Using in Keras\n",
    "Let's try to test the multi_processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "31aff62537d634ee34cd7752025d98615b1fc8e9"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, Conv2D, MaxPooling2D, BatchNormalization, Concatenate, ReLU, LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "5748b96367ce78ea9bce4a34a40a66dd5a49e945"
   },
   "outputs": [],
   "source": [
    "# credits: https://www.kaggle.com/guglielmocamporese/macro-f1-score-keras\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    #y_pred = K.round(y_pred)\n",
    "    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1-K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2d9bbc05fdf72242f8f98422ef9c480a353f4fe3"
   },
   "outputs": [],
   "source": [
    "# some basic useless model\n",
    "def create_model(input_shape):\n",
    "    \n",
    "    dropRate = 0.25\n",
    "    \n",
    "    init = Input(input_shape)\n",
    "    x = BatchNormalization(axis=-1)(init)\n",
    "    x = Conv2D(8, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(8, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(16, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    c1 = Conv2D(16, (3, 3), padding='same')(x)\n",
    "    c1 = ReLU()(c1)\n",
    "    c2 = Conv2D(16, (5, 5), padding='same')(x)\n",
    "    c2 = ReLU()(c2)\n",
    "    c3 = Conv2D(16, (7, 7), padding='same')(x)\n",
    "    c3 = ReLU()(c3)\n",
    "    c4 = Conv2D(16, (1, 1), padding='same')(x)\n",
    "    c4 = ReLU()(c4)\n",
    "    x = Concatenate()([c1, c2, c3, c4])\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    x = Conv2D(32, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    x = Conv2D(128, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    #x = Conv2D(256, (1, 1), activation='relu')(x)\n",
    "    #x = BatchNormalization(axis=-1)(x)\n",
    "    #x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    #x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(28)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(28)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    model = Model(init, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "a4982e06ba3bb236e2bb10b0b08683c93149a61c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 192, 192, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 192, 192, 4)  16          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 190, 190, 8)  296         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 190, 190, 8)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 190, 190, 8)  32          re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 188, 188, 8)  584         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 188, 188, 8)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 188, 188, 8)  32          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 186, 186, 16) 1168        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 186, 186, 16) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 186, 186, 16) 64          re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 93, 93, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 93, 93, 16)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 93, 93, 16)   2320        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 93, 93, 16)   6416        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 93, 93, 16)   12560       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 93, 93, 16)   272         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 93, 93, 16)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 93, 93, 16)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 93, 93, 16)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 93, 93, 16)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 93, 93, 64)   0           re_lu_4[0][0]                    \n",
      "                                                                 re_lu_5[0][0]                    \n",
      "                                                                 re_lu_6[0][0]                    \n",
      "                                                                 re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 93, 93, 64)   256         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 46, 46, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 46, 46, 64)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 44, 44, 32)   18464       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 44, 44, 32)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 44, 44, 32)   128         re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 22, 22, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 22, 22, 32)   0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 20, 20, 64)   18496       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 20, 20, 64)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 20, 64)   256         re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 10, 10, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 10, 10, 64)   0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 128)    73856       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 8, 8, 128)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 128)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 4, 4, 128)    0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2048)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 28)           57372       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 28)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 28)           112         re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 28)           0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 28)           812         dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 28)           0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 194,024\n",
      "Trainable params: 193,320\n",
      "Non-trainable params: 704\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(SHAPE)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-03),\n",
    "    metrics=['acc',f1])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "b8831e442b8f5461dfc37da4292f66e1684c84c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31072,) (31072, 28)\n",
      "(27964,) (27964, 28) (3108,) (3108, 28)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b40e55628651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathsTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathsVal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsVal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProteinDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathsTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mvg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProteinDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathsVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-711d64e63465>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, paths, labels, batch_size, shape, shuffle, use_cache, augment)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "paths, labels = getTrainDataset()\n",
    "\n",
    "# divide to \n",
    "keys = np.arange(paths.shape[0], dtype=np.int)  \n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(keys)\n",
    "lastTrainIndex = int((1-VAL_RATIO) * paths.shape[0])\n",
    "\n",
    "pathsTrain = paths[0:lastTrainIndex]\n",
    "labelsTrain = labels[0:lastTrainIndex]\n",
    "pathsVal = paths[lastTrainIndex:]\n",
    "labelsVal = labels[lastTrainIndex:]\n",
    "\n",
    "print(paths.shape, labels.shape)\n",
    "print(pathsTrain.shape, labelsTrain.shape, pathsVal.shape, labelsVal.shape)\n",
    "\n",
    "tg = ProteinDataGenerator(pathsTrain, labelsTrain, BATCH_SIZE, SHAPE, use_cache=True, augment = False, shuffle = False)\n",
    "vg = ProteinDataGenerator(pathsVal, labelsVal, BATCH_SIZE, SHAPE, use_cache=True, shuffle = False)\n",
    "\n",
    "# https://keras.io/callbacks/#modelcheckpoint\n",
    "checkpoint = ModelCheckpoint('./base.model', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "reduceLROnPlato = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78b72de7c88f7845f92a0cc93eeea7a5498cdb72"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "use_multiprocessing = False # DO NOT COMBINE MULTIPROCESSING WITH CACHE! \n",
    "workers = 1 # DO NOT COMBINE MULTIPROCESSING WITH CACHE! \n",
    "\n",
    "hist = model.fit_generator(\n",
    "    tg,\n",
    "    steps_per_epoch=len(tg),\n",
    "    validation_data=vg,\n",
    "    validation_steps=8,\n",
    "    epochs=epochs,\n",
    "    use_multiprocessing=use_multiprocessing,\n",
    "    workers=workers,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9237d3586a1bba31a3a5ef7f8e81a91827d94743"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5550c5553751b01fdfbdc3f3f310a819886d60aa"
   },
   "source": [
    "# Full validation\n",
    "Perform validation on full validation dataset. Choose appropriate prediction threshold maximalizing the validation F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a8a8fbd700822b1a5ede79d05989ba28c9df588"
   },
   "outputs": [],
   "source": [
    "bestModel = load_model('./base.model', custom_objects={'f1': f1}) #, 'f1_loss': f1_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f76abc9bb2cae0d85fc281c7582b3d642d2f1ff8"
   },
   "outputs": [],
   "source": [
    "fullValGen = vg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f03e6dcc106bad00e60cb6fefba20f84bf1f1be"
   },
   "outputs": [],
   "source": [
    "lastFullValPred = np.empty((0, 28))\n",
    "lastFullValLabels = np.empty((0, 28))\n",
    "for i in tqdm(range(len(fullValGen))): \n",
    "    im, lbl = fullValGen[i]\n",
    "    scores = bestModel.predict(im)\n",
    "    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n",
    "    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n",
    "print(lastFullValPred.shape, lastFullValLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8160f53f010b7a5eed1e50f2f24be4f6906b1389"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as off1\n",
    "rng = np.arange(0, 1, 0.001)\n",
    "f1s = np.zeros((rng.shape[0], 28))\n",
    "for j,t in enumerate(tqdm(rng)):\n",
    "    for i in range(28):\n",
    "        p = np.array(lastFullValPred[:,i]>t, dtype=np.int8)\n",
    "        scoref1 = off1(lastFullValLabels[:,i], p, average='binary')\n",
    "        f1s[j,i] = scoref1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6de70d7d65d4dd5b39af3221ceb4f89501ef2464"
   },
   "outputs": [],
   "source": [
    "print('Individual F1-scores for each class:')\n",
    "print(np.max(f1s, axis=0))\n",
    "print('Macro F1-score CV =', np.mean(np.max(f1s, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e48cd50cd31aac49f021d1266d5090228c054cda"
   },
   "outputs": [],
   "source": [
    "plt.plot(rng, f1s)\n",
    "T = np.empty(28)\n",
    "for i in range(28):\n",
    "    T[i] = rng[np.where(f1s[:,i] == np.max(f1s[:,i]))[0][0]]\n",
    "print('Probability threshold maximizing CV F1-score for each class:')\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "982c438bc6d8fa9208ba6050312abc9200f84799"
   },
   "outputs": [],
   "source": [
    "pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "testg = ProteinDataGenerator(pathsTest, labelsTest, BATCH_SIZE, SHAPE)\n",
    "submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "P = np.zeros((pathsTest.shape[0], 28))\n",
    "for i in tqdm(range(len(testg))):\n",
    "    images, labels = testg[i]\n",
    "    score = bestModel.predict(images)\n",
    "    P[i*BATCH_SIZE:i*BATCH_SIZE+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f52ecf0f38c5b61e97c31698d2f0db69a6a48184"
   },
   "outputs": [],
   "source": [
    "PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "668fd597b69767521b17a4a23320d3455d70bc73"
   },
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "    str_label = ''\n",
    "    \n",
    "    for col in range(PP.shape[1]):\n",
    "        if(PP[row, col] < T[col]):\n",
    "            str_label += ''\n",
    "        else:\n",
    "            str_label += str(col) + ' '\n",
    "    prediction.append(str_label.strip())\n",
    "    \n",
    "submit['Predicted'] = np.array(prediction)\n",
    "submit.to_csv('4channels_cnn_from_scratch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe7fe7be3814319d6ccace15f05e4477e239c71f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
